1. w2 and w3 are basically unchanged because the inputs x2 and x3 do not contribute to the output

2.  Findings
    Convergence rate In majority functions is fastest for Tanh and ReLu with distributions of [-1,1] 

    Convergence rate In one wire not functions is fastest for ReLu with distributions of [-1,1] and [0,1]

    Degeneration:

    Sigmoid degenerates because the weights become way to small, this leads to slower convergence

    Zero initialization causes degeneration mainly because the weights will stay the same for the whole training process
3. Problem on slide 9 on lecture 2

10. 
    1: According to the provided grammatical context, you can generate more sentences to be used as train examples
    2: Provides a set of rules that can be used to determine the structure of sentences in the CoLA dataset
    3: Test the validity of the current CoLA database
 

