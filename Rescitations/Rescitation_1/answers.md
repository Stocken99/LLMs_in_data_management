1. w2 and w3 are basically unchanged because the inputs x2 and x3 do not contribute to the output

2.  Findings
    Convergence rate In majority functions is fastest for Tanh and ReLu with distributions of [-1,1] 

    Convergence rate In one wire not functions is fastest for ReLu with distributions of [-1,1] and [0,1]

    Degeneration:

    Sigmoid degenerates because the weights become way to small, this leads to slower convergence

    Zero initialization causes degeneration mainly because the weights will stay the same for the whole training process
3. Problem on slide 9 on lecture 2

6. Combined result after five runs: Original: First = 217 Second = 315 Combined = 532 Average Combined = 106,4
                                    New     : First = 147 Second = 333 Combined = 475 Average Combined = 95
    Conclusion: For me, running the deeper neural network with 2 connected hidden layers degraded the final result with 11.4 percenage units

10. 
    1: According to the provided grammatical context, you can generate more sentences to be used as train examples
    2: Provides a set of rules that can be used to determine the structure of sentences in the CoLA dataset
    3: Test the validity of the current CoLA database
 

